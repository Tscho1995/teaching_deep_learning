{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#What-are-gradient-tapes?\" data-toc-modified-id=\"What-are-gradient-tapes?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What are gradient tapes?</a></span></li><li><span><a href=\"#A-first-simple-example\" data-toc-modified-id=\"A-first-simple-example-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>A first simple example</a></span></li><li><span><a href=\"#Which-tensors-are-watched?\" data-toc-modified-id=\"Which-tensors-are-watched?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Which tensors are watched?</a></span></li><li><span><a href=\"#Computing-the-derivative-for-more-than-one-tensor\" data-toc-modified-id=\"Computing-the-derivative-for-more-than-one-tensor-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Computing the derivative for more than one tensor</a></span></li><li><span><a href=\"#Computing-the-gradient-for-the-function-from-the-lecture\" data-toc-modified-id=\"Computing-the-gradient-for-the-function-from-the-lecture-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Computing the gradient for the function from the lecture</a></span></li><li><span><a href=\"#Fine-grained-control-over-which-variables-are-watched\" data-toc-modified-id=\"Fine-grained-control-over-which-variables-are-watched-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Fine grained control over which variables are watched</a></span></li><li><span><a href=\"#Automatic-differentiation-for-programs\" data-toc-modified-id=\"Automatic-differentiation-for-programs-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Automatic differentiation for programs</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "If you do not use Keras to build a neural network and train it using `fit()` in Tensorflow 2, you need to use \"gradient tapes\" directly in order to adjust the parameters of your model.\n",
    "\n",
    "So it seems to be a good idea first to understand what gradient tapes are.\n",
    "\n",
    "First make sure, you have the right TensorFlow version (>=2.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are gradient tapes?\n",
    "\n",
    "The TensorFlow website\n",
    "\n",
    "https://www.tensorflow.org/tutorials/customization/autodiff\n",
    "\n",
    "describes gradient tapes as follows:\n",
    "\n",
    "    TensorFlow provides the tf.GradientTape API for automatic differentiation - computing the gradient of a computation with respect to its input variables. Tensorflow \"records\" all operations executed inside the context of a tf.GradientTape onto a \"tape\". Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a \"recorded\" computation using reverse mode differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first simple example\n",
    "\n",
    "In the following we generate a TensorFlow op `y` which computes `y=x^2=x*x`.\n",
    "\n",
    "Let us assume we want to compute the derivative of `y` with respect to the input variable `x` (the result should be `2*x`, right?)\n",
    "\n",
    "So if we compute the derivative dy/dx at x=3.0, the result should be 6.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as t:\n",
    "  t.watch(x)\n",
    "  y = x * x\n",
    "dy_dx = t.gradient(y, x) # Will compute to 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the statement `t.watch(x)` is important. If we omit it, the tape has not stored any information in order to compute the gradient of dy/dx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as t:\n",
    "  #t.watch(x)  # <-- we now omit this statement\n",
    "  y = x * x\n",
    "dy_dx = t.gradient(y, x) # Will compute to 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which tensors are watched?\n",
    "\n",
    "The TensorFlow website says:\n",
    "\n",
    "    Trainable variables (created by tf.Variable or tf.compat.v1.get_variable, where trainable=True is default in both cases) are automatically watched. Tensors can be manually watched by invoking the watch method on this context manager.\n",
    "    \n",
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as t:\n",
    "  #t.watch(x)  # <-- we now omit this statement\n",
    "  y = x * x\n",
    "dy_dx = t.gradient(y, x) # Will compute to 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the statement is right! Though we did not tell the tape to explicitly watch `x`, we have no the desired information on the tape in order to compute the gradient dy/dx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the derivative for more than one tensor\n",
    "\n",
    "In the following example we do not only compute dz/dx, but also dy/dx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "  t.watch(x)\n",
    "  y = x * x # y=x^2\n",
    "  z = y * y # z=y^2=(x^2)^2=x^4\n",
    "\n",
    "dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x=3 --> 4*27=108)\n",
    "dy_dx = t.gradient(y, x)  # 6.0 (2*x at x=3 --> 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=32, shape=(), dtype=float32, numpy=108.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=36, shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the flag `Persistent=True`?\n",
    "\n",
    "Here is the description why we need this flag to be set:\n",
    "\n",
    "    By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also ask the tape to give us the derivative of the output with respect to some *intermediate* node (op, tensor), e.g. dz/dy = 2*y\n",
    "\n",
    "For x=3.0, y=x*x=9.0, so dz/dy=2*y at y=9 will be 18.0, right?\n",
    "\n",
    "Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "  t.watch(x)\n",
    "  y = x * x # y=x^2\n",
    "  z = y * y # z=y^2=(x^2)^2=x^4\n",
    "\n",
    "dz_dy = t.gradient(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dz_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the gradient for the function from the lecture\n",
    "\n",
    "In the lecture we considered the function\n",
    "\n",
    "f(x1,x2) := x1*x2 + sin(x1)\n",
    "\n",
    "Now let us compute the gradient of f which is (df/dx1, df/dx2) using a gradient tape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant(1.0)\n",
    "x2 = tf.constant(2.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x1)\n",
    "    t.watch(x2)\n",
    "    w3 = x1*x2\n",
    "    w4 = tf.math.sin(x1)\n",
    "    w5 = w3*w4\n",
    "\n",
    "dw5_dx1 = t.gradient(w5, x1)\n",
    "dw5_dx2 = t.gradient(w5, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=67, shape=(), dtype=float32, numpy=2.7635465>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw5_dx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=72, shape=(), dtype=float32, numpy=0.84147096>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw5_dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with the results we got on slide 126. It is exactly the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine grained control over which variables are watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_a = tf.Variable(2.0)\n",
    "variable_b = tf.Variable(4.0)\n",
    "with tf.GradientTape(watch_accessed_variables=False, persistent=True) as t:\n",
    "  t.watch(variable_a)\n",
    "  y = variable_a ** 2  # Gradients will be available for `variable_a`.\n",
    "  z = variable_b ** 3  # No gradients will be available since `variable_b` is\n",
    "                       # not being watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print( t.gradient(y,variable_a) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print( t.gradient(z,variable_b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation for programs\n",
    "\n",
    "Tapes can record operations as they are executed. This allows to compute the derivative of functions computed with \"normal\" Python control flow structures such as if statements and while loops.\n",
    "\n",
    "Here is an simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x, y):      \n",
    "  if y >= 7:\n",
    "        output = x+x+x\n",
    "        return output\n",
    "  output=1.0\n",
    "  for i in range(y):\n",
    "      output = output * x\n",
    "  return output\n",
    "\n",
    "def compute_gradient(x, y):\n",
    "  with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    out = foo(x, y)\n",
    "  return t.gradient(out, x)\n",
    "\n",
    "x = tf.constant(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=113, shape=(), dtype=float32, numpy=12.0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(x,3) # x^3 derived is 3*x^2. With x=2.0, we get 3*4=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=117, shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(x,12) # for foo(x,12)\n",
    "                       # the if statement is true\n",
    "                       # and thus the function value foo(x,12)=x+x+x=3x.\n",
    "                       # The derivative is therefore 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=138, shape=(), dtype=float32, numpy=192.0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(x,6) # x^3 derived is 6*x^5. With x=2.0, we get 6*(2^5) = 6*32=192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
